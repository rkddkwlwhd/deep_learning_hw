{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84353850-deb9-4964-89cc-c15b8920ec02",
   "metadata": {},
   "source": [
    "# HW_1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c0bf98-24f7-41af-8bef-bb17e12f5b5c",
   "metadata": {},
   "source": [
    "## a_tensor_initalization.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dfdccadb-b8f4-4d54-95e7-b87cbbf04bf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n",
      "cpu\n",
      "False\n",
      "torch.Size([3])\n",
      "torch.Size([3])\n",
      "################################################## 1\n",
      "torch.int64\n",
      "cpu\n",
      "False\n",
      "torch.Size([3])\n",
      "torch.Size([3])\n",
      "################################################## 2\n",
      "torch.Size([]) 0\n",
      "torch.Size([1]) 1\n",
      "torch.Size([5]) 1\n",
      "torch.Size([5, 1]) 2\n",
      "torch.Size([3, 2]) 2\n",
      "torch.Size([3, 2, 1]) 3\n",
      "torch.Size([3, 1, 2, 1]) 4\n",
      "torch.Size([3, 1, 2, 3]) 4\n",
      "torch.Size([3, 1, 2, 3, 1]) 5\n",
      "torch.Size([4, 5]) 2\n",
      "torch.Size([4, 1, 5]) 3\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# [핵심코드] torch.Tensor 클래스 사용 예시\n",
    "t1 = torch.Tensor([1, 2, 3], device='cpu')  # [핵심코드] CPU에서 1차원 텐서를 생성\n",
    "print(t1.dtype)   # >>> torch.float32\n",
    "print(t1.device)  # >>> cpu\n",
    "print(t1.requires_grad)  # >>> False\n",
    "print(t1.size())  # [핵심코드] 텐서의 크기 확인, torch.Size([3])\n",
    "print(t1.shape)   # [핵심코드] 텐서의 형태 확인, torch.Size([3])\n",
    "\n",
    "# [핵심코드] GPU 장치를 사용할 경우\n",
    "# t1_cuda = t1.to(torch.device('cuda'))\n",
    "# 또는 간단한 방법으로\n",
    "# t1_cuda = t1.cuda()\n",
    "t1_cpu = t1.cpu()  # [핵심코드] CPU로 텐서를 이동\n",
    "\n",
    "print(\"#\" * 50, 1)\n",
    "\n",
    "# [핵심코드] torch.tensor 함수 사용 예시\n",
    "t2 = torch.tensor([1, 2, 3], device='cpu')  # [핵심코드] CPU에서 정수형 텐서를 생성\n",
    "print(t2.dtype)  # >>> torch.int64\n",
    "print(t2.device)  # >>> cpu\n",
    "print(t2.requires_grad)  # >>> False\n",
    "print(t2.size())  # [핵심코드] 텐서의 크기 확인, torch.Size([3])\n",
    "print(t2.shape)  # [핵심코드] 텐서의 형태 확인, torch.Size([3])\n",
    "\n",
    "# [핵심코드] GPU 장치를 사용할 경우\n",
    "# t2_cuda = t2.to(torch.device('cuda'))\n",
    "# 또는 간단한 방법으로\n",
    "# t2_cuda = t2.cuda()\n",
    "t2_cpu = t2.cpu()  # [핵심코드] CPU로 텐서를 이동\n",
    "\n",
    "print(\"#\" * 50, 2)\n",
    "\n",
    "# [핵심코드] 다양한 형태의 텐서 생성 및 차원 확인\n",
    "a1 = torch.tensor(1)  # [핵심코드] 0차원 텐서\n",
    "print(a1.shape, a1.ndim)\n",
    "\n",
    "a2 = torch.tensor([1])  # [핵심코드] 1차원 텐서\n",
    "print(a2.shape, a2.ndim)\n",
    "\n",
    "a3 = torch.tensor([1, 2, 3, 4, 5])  # [핵심코드] 1차원 텐서\n",
    "print(a3.shape, a3.ndim)\n",
    "\n",
    "a4 = torch.tensor([[1], [2], [3], [4], [5]])  # [핵심코드] 2차원 텐서\n",
    "print(a4.shape, a4.ndim)\n",
    "\n",
    "a5 = torch.tensor([  # [핵심코드] 2차원 텐서\n",
    "    [1, 2],\n",
    "    [3, 4],\n",
    "    [5, 6]\n",
    "])\n",
    "print(a5.shape, a5.ndim)\n",
    "\n",
    "a6 = torch.tensor([  # [핵심코드] 3차원 텐서\n",
    "    [[1], [2]],\n",
    "    [[3], [4]],\n",
    "    [[5], [6]]\n",
    "])\n",
    "print(a6.shape, a6.ndim)\n",
    "\n",
    "a7 = torch.tensor([  # [핵심코드] 4차원 텐서\n",
    "    [[[1], [2]]],\n",
    "    [[[3], [4]]],\n",
    "    [[[5], [6]]]\n",
    "])\n",
    "print(a7.shape, a7.ndim)\n",
    "\n",
    "a8 = torch.tensor([  # [핵심코드] 4차원 텐서, 각 요소가 2차원 배열인 경우\n",
    "    [[[1, 2, 3], [2, 3, 4]]],\n",
    "    [[[3, 1, 1], [4, 4, 5]]],\n",
    "    [[[5, 6, 2], [6, 3, 1]]]\n",
    "])\n",
    "print(a8.shape, a8.ndim)\n",
    "\n",
    "a9 = torch.tensor([  # [핵심코드] 5차원 텐서\n",
    "    [[[[1], [2], [3]], [[2], [3], [4]]]],\n",
    "    [[[[3], [1], [1]], [[4], [4], [5]]]],\n",
    "    [[[[5], [6], [2]], [[6], [3], [1]]]]\n",
    "])\n",
    "print(a9.shape, a9.ndim)\n",
    "\n",
    "a10 = torch.tensor([  # [핵심코드] 2차원 텐서\n",
    "    [1, 2, 3, 4, 5],\n",
    "    [1, 2, 3, 4, 5],\n",
    "    [1, 2, 3, 4, 5],\n",
    "    [1, 2, 3, 4, 5],\n",
    "])\n",
    "print(a10.shape, a10.ndim)\n",
    "\n",
    "a10 = torch.tensor([  # [핵심코드] 3차원 텐서\n",
    "    [[1, 2, 3, 4, 5]],\n",
    "    [[1, 2, 3, 4, 5]],\n",
    "    [[1, 2, 3, 4, 5]],\n",
    "    [[1, 2, 3, 4, 5]],\n",
    "])\n",
    "print(a10.shape, a10.ndim)\n",
    "\n",
    "# 기술적 사항\n",
    "# 1. torch.Tensor와 torch.tensor의 차이를 이해할 수 있다. 전자는 기본 텐서 생성, 후자는 데이터 타입 지정이 가능하다.\n",
    "# 2. 다양한 차원의 텐서를 생성하고 각 텐서의 차원(ndim)과 형태(shape)를 확인하는 방법을 익혔다.\n",
    "# 3. CUDA를 활용하여 GPU에서 텐서를 다루는 방법을 설명하였다.\n",
    "\n",
    "# 고찰\n",
    "# 1. 텐서의 차원과 형태를 이해하는 것은 딥러닝 모델을 구성하는 데 매우 중요하다. 각 차원은 데이터의 구조를 나타내며, 올바른 차원으로 모델을 설계해야 한다.\n",
    "# 2. CPU와 GPU 간의 텐서 이동은 성능 최적화를 위해 필수적이다. 대규모 데이터 처리 시 GPU를 활용해야 하므로 이를 고려해야 한다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7bb1e6f2-65bd-4b5b-86a6-7a2ce44ba620",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "expected sequence of length 3 at dim 3 (got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 잘못된 텐서 생성 예시\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m a11 \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# [핵심코드] 4차원 텐서 생성 시 오류 발생\u001b[39;49;00m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# 이 부분에서 오류 발생, 각 내부 리스트의 길이가 불일치\u001b[39;49;00m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mValueError\u001b[0m: expected sequence of length 3 at dim 3 (got 2)"
     ]
    }
   ],
   "source": [
    "# 잘못된 텐서 생성 예시\n",
    "a11 = torch.tensor([  # [핵심코드] 4차원 텐서 생성 시 오류 발생\n",
    "    [[[1, 2, 3], [4, 5]]],  # 이 부분에서 오류 발생, 각 내부 리스트의 길이가 불일치\n",
    "    [[[1, 2, 3], [4, 5]]],\n",
    "    [[[1, 2, 3], [4, 5]]],\n",
    "    [[[1, 2, 3], [4, 5]]],\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f783b48-fe0a-47a2-ac87-3d8921e6e906",
   "metadata": {},
   "source": [
    "## b_tensor_initalization_copy.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "de608163-5120-4761-9d76-126c3849fb44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 3.])\n",
      "tensor([1, 2, 3])\n",
      "tensor([1, 2, 3])\n",
      "####################################################################################################\n",
      "tensor([1., 2., 3.])\n",
      "tensor([1, 2, 3], dtype=torch.int32)\n",
      "tensor([100,   2,   3], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# [핵심코드] 리스트를 텐서로 변환하는 방법\n",
    "l1 = [1, 2, 3]\n",
    "t1 = torch.Tensor(l1)  # [핵심코드] 기본 텐서 생성\n",
    "\n",
    "l2 = [1, 2, 3]\n",
    "t2 = torch.tensor(l2)  # [핵심코드] 데이터 타입이 자동으로 설정되는 텐서 생성\n",
    "\n",
    "l3 = [1, 2, 3]\n",
    "t3 = torch.as_tensor(l3)  # [핵심코드] 기존 데이터를 참조하는 텐서 생성\n",
    "\n",
    "# 리스트의 첫 번째 요소를 변경\n",
    "l1[0] = 100\n",
    "l2[0] = 100\n",
    "l3[0] = 100\n",
    "\n",
    "# 각 텐서 출력\n",
    "print(t1)  # [핵심코드] t1은 l1의 복사본으로, l1의 변경이 반영되지 않음\n",
    "print(t2)  # [핵심코드] t2는 l2의 복사본으로, l2의 변경이 반영되지 않음\n",
    "print(t3)  # [핵심코드] t3은 l3의 참조로, l3의 변경이 반영됨\n",
    "\n",
    "print(\"#\" * 100)\n",
    "\n",
    "# [핵심코드] Numpy 배열을 텐서로 변환하는 방법\n",
    "l4 = np.array([1, 2, 3])\n",
    "t4 = torch.Tensor(l4)  # [핵심코드] Numpy 배열을 복사하여 텐서 생성\n",
    "\n",
    "l5 = np.array([1, 2, 3])\n",
    "t5 = torch.tensor(l5)  # [핵심코드] Numpy 배열을 복사하여 텐서 생성\n",
    "\n",
    "l6 = np.array([1, 2, 3])\n",
    "t6 = torch.as_tensor(l6)  # [핵심코드] Numpy 배열을 참조하는 텐서 생성\n",
    "\n",
    "# Numpy 배열의 첫 번째 요소를 변경\n",
    "l4[0] = 100\n",
    "l5[0] = 100\n",
    "l6[0] = 100\n",
    "\n",
    "# 각 텐서 출력\n",
    "print(t4)  # [핵심코드] t4는 l4의 복사본으로, l4의 변경이 반영되지 않음\n",
    "print(t5)  # [핵심코드] t5는 l5의 복사본으로, l5의 변경이 반영되지 않음\n",
    "print(t6)  # [핵심코드] t6은 l6의 참조로, l6의 변경이 반영됨\n",
    "\n",
    "# 기술적 사항\n",
    "# 1. torch.Tensor와 torch.tensor는 데이터를 복사하여 새로운 텐서를 생성하며, torch.as_tensor는 기존 데이터를 참조하여 텐서를 생성한다.\n",
    "# 2. 리스트나 Numpy 배열의 요소를 변경할 때, 텐서의 내용이 어떻게 반영되는지를 이해할 수 있다.\n",
    "\n",
    "# 고찰\n",
    "# 1. 데이터의 복사 여부에 따라 텐서의 변경 사항이 달라지므로, 메모리 사용 및 성능 최적화 측면에서 상황에 맞는 함수 선택이 중요하다.\n",
    "# 2. Numpy 배열과 PyTorch 텐서 간의 상호작용을 이해하는 것은 데이터 전처리 및 모델 학습 과정에서 매우 중요하다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf5ef98-24d3-433c-b48a-8f8050b78e33",
   "metadata": {},
   "source": [
    "## c_tensor_initalization_constant_values.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "015db49d-b16b-4949-be73-9618a348643a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "tensor([0., 0., 0., 0., 0., 0.])\n",
      "tensor([0., 0., 0., 0., 0., 0.])\n",
      "tensor([2.3694e-38, 1.4013e-45, 0.0000e+00, 0.0000e+00])\n",
      "tensor([7.0065e-45, 0.0000e+00, 0.0000e+00, 0.0000e+00])\n",
      "tensor([[1., 0., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 0., 1.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# [핵심코드] 텐서를 생성하는 다양한 방법: 모든 요소가 1인 텐서\n",
    "t1 = torch.ones(size=(5,))  # [핵심코드] 크기가 5인 1로 채워진 텐서 생성\n",
    "t1_like = torch.ones_like(input=t1)  # [핵심코드] t1과 같은 크기의 1로 채워진 텐서 생성\n",
    "print(t1)  # >>> tensor([1., 1., 1., 1., 1.])\n",
    "print(t1_like)  # >>> tensor([1., 1., 1., 1., 1.])\n",
    "\n",
    "# [핵심코드] 텐서를 생성하는 다양한 방법: 모든 요소가 0인 텐서\n",
    "t2 = torch.zeros(size=(6,))  # [핵심코드] 크기가 6인 0으로 채워진 텐서 생성\n",
    "t2_like = torch.zeros_like(input=t2)  # [핵심코드] t2와 같은 크기의 0으로 채워진 텐서 생성\n",
    "print(t2)  # >>> tensor([0., 0., 0., 0., 0., 0.])\n",
    "print(t2_like)  # >>> tensor([0., 0., 0., 0., 0., 0.])\n",
    "\n",
    "# [핵심코드] 텐서를 생성하는 방법: 비어 있는 텐서\n",
    "t3 = torch.empty(size=(4,))  # [핵심코드] 크기가 4인 비어 있는 텐서 생성\n",
    "t3_like = torch.empty_like(input=t3)  # [핵심코드] t3와 같은 크기의 비어 있는 텐서 생성\n",
    "print(t3)  # >>> tensor([0., 0., 0., 0.]) (초기값은 정해져 있지 않음)\n",
    "print(t3_like)  # >>> tensor([0., 0., 0., 0.]) (초기값은 정해져 있지 않음)\n",
    "\n",
    "# [핵심코드] 단위 행렬 생성\n",
    "t4 = torch.eye(n=3)  # [핵심코드] 3x3 단위 행렬 생성\n",
    "print(t4)\n",
    "\n",
    "# 기술적 사항\n",
    "# 1. torch.ones, torch.zeros, torch.empty, torch.eye와 같은 함수를 사용하여 다양한 형태의 텐서를 쉽게 생성할 수 있다.\n",
    "# 2. ones_like 및 zeros_like 함수는 기존 텐서와 같은 크기의 텐서를 생성하는 데 유용하다.\n",
    "\n",
    "# 고찰\n",
    "# 1. 텐서의 초기화 방식에 따라 메모리 효율성을 고려할 수 있으며, 필요한 경우에 맞는 초기화 방법을 선택해야 한다.\n",
    "# 2. 단위 행렬은 선형 대수에서 매우 중요한 역할을 하며, 텐서 연산에서 자주 사용되므로 이를 이해하는 것이 중요하다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293b50bf-e4cb-41c0-97d6-25ebccf64e45",
   "metadata": {},
   "source": [
    "## d_tensor_initalization_random_value.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dfa67f1e-2d32-4aa0-b50e-9effb573cdfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[19, 14]])\n",
      "tensor([[0.9280, 0.1564, 0.2245]])\n",
      "tensor([[-0.1218,  0.6751,  1.9434]])\n",
      "tensor([[ 9.1712, 10.5786],\n",
      "        [ 9.0025, 11.8824],\n",
      "        [ 9.6794,  9.6363]])\n",
      "tensor([0.0000, 2.5000, 5.0000])\n",
      "tensor([0, 1, 2, 3, 4])\n",
      "##############################\n",
      "tensor([[0.3126, 0.3791, 0.3087],\n",
      "        [0.0736, 0.4216, 0.0691]])\n",
      "tensor([[0.2332, 0.4047, 0.2162],\n",
      "        [0.9927, 0.4128, 0.5938]])\n",
      "\n",
      "tensor([[0.3126, 0.3791, 0.3087],\n",
      "        [0.0736, 0.4216, 0.0691]])\n",
      "tensor([[0.2332, 0.4047, 0.2162],\n",
      "        [0.9927, 0.4128, 0.5938]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# [핵심코드] 정수로 구성된 랜덤 텐서 생성\n",
    "t1 = torch.randint(low=10, high=20, size=(1, 2))  # [핵심코드] 10 이상 20 미만의 정수를 랜덤으로 생성\n",
    "print(t1)\n",
    "\n",
    "# [핵심코드] 균일 분포에서 샘플링한 랜덤 텐서 생성\n",
    "t2 = torch.rand(size=(1, 3))  # [핵심코드] 0과 1 사이의 랜덤 실수 생성\n",
    "print(t2)\n",
    "\n",
    "# [핵심코드] 정규 분포에서 샘플링한 랜덤 텐서 생성\n",
    "t3 = torch.randn(size=(1, 3))  # [핵심코드] 평균 0, 표준편차 1의 정규 분포에서 랜덤 실수 생성\n",
    "print(t3)\n",
    "\n",
    "# [핵심코드] 정규 분포에 따라 랜덤 텐서 생성\n",
    "t4 = torch.normal(mean=10.0, std=1.0, size=(3, 2))  # [핵심코드] 평균 10, 표준편차 1인 정규 분포에서 샘플링\n",
    "print(t4)\n",
    "\n",
    "# [핵심코드] 균일 간격의 텐서 생성\n",
    "t5 = torch.linspace(start=0.0, end=5.0, steps=3)  # [핵심코드] 0과 5 사이를 3개의 균일한 간격으로 나눈 텐서 생성\n",
    "print(t5)\n",
    "\n",
    "# [핵심코드] 일정 간격의 정수 텐서 생성\n",
    "t6 = torch.arange(5)  # [핵심코드] 0부터 4까지의 정수로 구성된 텐서 생성\n",
    "print(t6)\n",
    "\n",
    "print(\"#\" * 30)\n",
    "\n",
    "# [핵심코드] 랜덤 텐서의 시드 설정\n",
    "torch.manual_seed(1729)  # [핵심코드] 랜덤 시드를 설정하여 재현 가능성 확보\n",
    "random1 = torch.rand(2, 3)  # [핵심코드] 2x3 크기의 랜덤 텐서 생성\n",
    "print(random1)\n",
    "\n",
    "random2 = torch.rand(2, 3)  # [핵심코드] 또 다른 2x3 크기의 랜덤 텐서 생성\n",
    "print(random2)\n",
    "\n",
    "print()\n",
    "\n",
    "# [핵심코드] 동일한 시드로 랜덤 텐서 생성\n",
    "torch.manual_seed(1729)  # [핵심코드] 동일한 시드를 설정하여 동일한 랜덤 값을 생성\n",
    "random3 = torch.rand(2, 3)  # [핵심코드] 2x3 크기의 랜덤 텐서 생성\n",
    "print(random3)\n",
    "\n",
    "random4 = torch.rand(2, 3)  # [핵심코드] 또 다른 2x3 크기의 랜덤 텐서 생성\n",
    "print(random4)\n",
    "\n",
    "# 기술적 사항\n",
    "# 1. 다양한 방법으로 랜덤 텐서를 생성할 수 있으며, 각 방법은 서로 다른 분포에서 샘플링한다.\n",
    "# 2. torch.manual_seed를 사용하여 랜덤 텐서 생성의 재현성을 확보할 수 있다.\n",
    "\n",
    "# 고찰\n",
    "# 1. 랜덤 텐서 생성은 딥러닝 모델에서 데이터 샘플링, 초기화 등에 많이 사용되므로 이해하고 활용하는 것이 중요하다.\n",
    "# 2. 랜덤 시드를 설정하는 것은 실험의 재현성을 보장하기 위해 필수적이며, 이를 통해 결과를 비교할 수 있다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "844795b2-fab2-4850-b77b-a54513415757",
   "metadata": {},
   "source": [
    "## e_tensor_type_conversion.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6bf704d4-7aec-4310-96ee-befed205ec91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n",
      "tensor([[1, 1, 1],\n",
      "        [1, 1, 1]], dtype=torch.int16)\n",
      "tensor([[18.0429,  7.2532, 19.6519],\n",
      "        [10.8626,  2.1505, 19.6913]], dtype=torch.float64)\n",
      "tensor([[1, 1, 1],\n",
      "        [1, 1, 1]], dtype=torch.int32)\n",
      "torch.float64\n",
      "torch.int16\n",
      "torch.float64\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# [핵심코드] 기본 텐서 생성 및 데이터 타입 확인\n",
    "a = torch.ones((2, 3))  # [핵심코드] 크기 (2, 3)인 1로 채워진 텐서 생성\n",
    "print(a.dtype)  # [핵심코드] 기본 데이터 타입은 float32\n",
    "\n",
    "# [핵심코드] 특정 데이터 타입을 가진 텐서 생성\n",
    "b = torch.ones((2, 3), dtype=torch.int16)  # [핵심코드] int16 타입의 1로 채워진 텐서 생성\n",
    "print(b)\n",
    "\n",
    "# [핵심코드] 랜덤 텐서 생성 후 스케일링\n",
    "c = torch.rand((2, 3), dtype=torch.float64) * 20.  # [핵심코드] float64 타입으로 0~20 사이의 랜덤 텐서 생성\n",
    "print(c)\n",
    "\n",
    "# [핵심코드] 데이터 타입 변환\n",
    "d = b.to(torch.int32)  # [핵심코드] int16 타입의 텐서를 int32로 변환\n",
    "print(d)\n",
    "\n",
    "# [핵심코드] double과 short 타입의 텐서 생성\n",
    "double_d = torch.ones(10, 2, dtype=torch.double)  # [핵심코드] double 타입의 10x2 텐서 생성\n",
    "short_e = torch.tensor([[1, 2]], dtype=torch.short)  # [핵심코드] short 타입의 1x2 텐서 생성\n",
    "\n",
    "# [핵심코드] 제로 텐서를 생성한 후 타입 설정\n",
    "double_d = torch.zeros(10, 2).double()  # [핵심코드] 10x2 크기의 제로 텐서를 double 타입으로 변환\n",
    "short_e = torch.ones(10, 2).short()  # [핵심코드] 10x2 크기의 1로 채워진 텐서를 short 타입으로 생성\n",
    "\n",
    "# [핵심코드] 데이터 타입 출력\n",
    "double_d = torch.zeros(10, 2).to(torch.double)  # [핵심코드] double 타입의 제로 텐서 생성\n",
    "short_e = torch.ones(10, 2).to(dtype=torch.short)  # [핵심코드] short 타입의 1로 채워진 텐서 생성\n",
    "\n",
    "print(double_d.dtype)  # [핵심코드] double 타입 확인\n",
    "print(short_e.dtype)  # [핵심코드] short 타입 확인\n",
    "\n",
    "# [핵심코드] 랜덤 double 텐서 생성 및 타입 변환\n",
    "double_f = torch.rand(5, dtype=torch.double)  # [핵심코드] double 타입의 랜덤 텐서 생성\n",
    "short_g = double_f.to(torch.short)  # [핵심코드] double 텐서를 short 타입으로 변환\n",
    "print((double_f * short_g).dtype)  # [핵심코드] 두 텐서의 곱의 데이터 타입 출력\n",
    "\n",
    "# 기술적 사항\n",
    "# 1. 텐서 생성 시 데이터 타입을 지정할 수 있으며, 다양한 타입 간 변환이 가능하다.\n",
    "# 2. torch.ones, torch.zeros, torch.rand와 같은 함수는 데이터 타입을 설정할 수 있는 다양한 옵션을 제공한다.\n",
    "\n",
    "# 고찰\n",
    "# 1. 데이터 타입은 메모리 사용과 연산 속도에 큰 영향을 미치므로, 적절한 타입 선택이 중요하다.\n",
    "# 2. 다양한 데이터 타입 간의 변환을 이해하는 것은 텐서 연산에서의 유연성을 높여준다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08610d8-349f-428e-b645-78e950029be6",
   "metadata": {},
   "source": [
    "## f_tensor_operations.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7a6c48e4-cfc9-47aa-92b4-8fa14c92c597",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 2., 2.],\n",
      "        [2., 2., 2.]])\n",
      "tensor([[2., 2., 2.],\n",
      "        [2., 2., 2.]])\n",
      "##############################\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "##############################\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "##############################\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 텐서 생성 및 덧셈 연산\n",
    "t1 = torch.ones(size=(2, 3))  # 크기 (2, 3)인 1로 채워진 텐서 생성\n",
    "t2 = torch.ones(size=(2, 3))  # 또 다른 크기 (2, 3)인 1로 채워진 텐서 생성\n",
    "t3 = torch.add(t1, t2)  # torch.add를 사용한 덧셈\n",
    "t4 = t1 + t2  # 연산자 오버로딩을 통한 덧셈\n",
    "print(t3)  # 덧셈 결과 출력\n",
    "print(t4)  # 덧셈 결과 출력\n",
    "\n",
    "print(\"#\" * 30)\n",
    "\n",
    "# 텐서 뺄셈 연산\n",
    "t5 = torch.sub(t1, t2)  # torch.sub를 사용한 뺄셈\n",
    "t6 = t1 - t2  # 연산자 오버로딩을 통한 뺄셈\n",
    "print(t5)  # 뺄셈 결과 출력\n",
    "print(t6)  # 뺄셈 결과 출력\n",
    "\n",
    "print(\"#\" * 30)\n",
    "\n",
    "# 텐서 곱셈 연산\n",
    "t7 = torch.mul(t1, t2)  # torch.mul을 사용한 곱셈\n",
    "t8 = t1 * t2  # 연산자 오버로딩을 통한 곱셈\n",
    "print(t7)  # 곱셈 결과 출력\n",
    "print(t8)  # 곱셈 결과 출력\n",
    "\n",
    "print(\"#\" * 30)\n",
    "\n",
    "# 텐서 나눗셈 연산\n",
    "t9 = torch.div(t1, t2)  # torch.div를 사용한 나눗셈\n",
    "t10 = t1 / t2  # 연산자 오버로딩을 통한 나눗셈\n",
    "print(t9)  # 나눗셈 결과 출력\n",
    "print(t10)  # 나눗셈 결과 출력\n",
    "\n",
    "# 기술적 사항\n",
    "# 1. PyTorch에서는 다양한 방식으로 텐서 간의 기본 연산(덧셈, 뺄셈, 곱셈, 나눗셈)을 수행할 수 있다.\n",
    "# 2. torch.add, torch.sub, torch.mul, torch.div와 같은 함수는 명시적인 연산을 제공하며, 연산자 오버로딩을 통해 간결하게 표현할 수 있다.\n",
    "\n",
    "# 고찰\n",
    "# 1. 텐서 연산은 딥러닝 모델의 기본적인 구성 요소이며, 연산의 효율성과 정확성을 위해 다양한 방법을 이해하고 활용하는 것이 중요하다.\n",
    "# 2. 연산 방식에 따라 코드의 가독성이 달라질 수 있으므로, 상황에 맞는 방법을 선택하는 것이 중요하다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0db1386-7471-4c45-9ad8-3796764c0a73",
   "metadata": {},
   "source": [
    "## g_tensor_operations_mm.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d9f9d90d-5b45-42dd-9920-8ee5abfe9d4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(7) torch.Size([])\n",
      "tensor([[1.6750, 2.2840],\n",
      "        [0.0956, 1.0294]]) torch.Size([2, 2])\n",
      "torch.Size([10, 3, 5])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# [핵심코드] 벡터의 내적 연산\n",
    "t1 = torch.dot(\n",
    "  torch.tensor([2, 3]), torch.tensor([2, 1])  # 두 벡터의 내적 계산\n",
    ")\n",
    "print(t1, t1.size())  # 결과 및 크기 출력\n",
    "\n",
    "# [핵심코드] 행렬 곱셈 연산\n",
    "t2 = torch.randn(2, 3)  # 2x3 크기의 랜덤 텐서 생성\n",
    "t3 = torch.randn(3, 2)  # 3x2 크기의 랜덤 텐서 생성\n",
    "t4 = torch.mm(t2, t3)  # 행렬 곱셈\n",
    "print(t4, t4.size())  # 결과 및 크기 출력\n",
    "\n",
    "# [핵심코드] 배치 행렬 곱셈 연산\n",
    "t5 = torch.randn(10, 3, 4)  # 10개의 3x4 크기 배치 텐서 생성\n",
    "t6 = torch.randn(10, 4, 5)  # 10개의 4x5 크기 배치 텐서 생성\n",
    "t7 = torch.bmm(t5, t6)  # 배치 행렬 곱셈\n",
    "print(t7.size())  # 결과 크기 출력\n",
    "\n",
    "# 기술적 사항\n",
    "# 1. torch.dot()은 두 벡터 간의 내적을 계산하는 함수이다.\n",
    "# 2. torch.mm()은 두 행렬 간의 곱셈을 수행하며, torch.bmm()은 배치 행렬 곱셈을 수행한다.\n",
    "\n",
    "# 고찰\n",
    "# 1. 내적, 행렬 곱셈 및 배치 행렬 곱셈은 딥러닝 모델에서 중요한 연산으로, 이들 연산의 효율성을 이해하는 것이 중요하다.\n",
    "# 2. PyTorch는 다양한 텐서 연산을 지원하여 복잡한 계산을 효율적으로 수행할 수 있게 해준다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "887ce002-a34b-4762-807b-52c054b40ef5",
   "metadata": {},
   "source": [
    "## h_tensor_operations_matmul.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1a574225-75cc-4806-8db4-07f2fa2bce38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([])\n",
      "torch.Size([3])\n",
      "torch.Size([10, 3])\n",
      "torch.Size([10, 3, 5])\n",
      "torch.Size([10, 3, 5])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# [핵심코드] 벡터 간의 내적: dot product\n",
    "t1 = torch.randn(3)\n",
    "t2 = torch.randn(3)\n",
    "print(torch.matmul(t1, t2).size())  # 결과 크기: torch.Size([])\n",
    "\n",
    "# [핵심코드] 행렬과 벡터 간의 연산: broadcasted dot\n",
    "t3 = torch.randn(3, 4)\n",
    "t4 = torch.randn(4)\n",
    "print(torch.matmul(t3, t4).size())  # 결과 크기: torch.Size([3])\n",
    "\n",
    "# [핵심코드] 배치된 행렬과 벡터 간의 연산: broadcasted dot\n",
    "t5 = torch.randn(10, 3, 4)\n",
    "t6 = torch.randn(4)\n",
    "print(torch.matmul(t5, t6).size())  # 결과 크기: torch.Size([10, 3])\n",
    "\n",
    "# [핵심코드] 배치된 행렬 간의 연산: bmm\n",
    "t7 = torch.randn(10, 3, 4)\n",
    "t8 = torch.randn(10, 4, 5)\n",
    "print(torch.matmul(t7, t8).size())  # 결과 크기: torch.Size([10, 3, 5])\n",
    "\n",
    "# [핵심코드] 배치된 행렬과 행렬 간의 연산: bmm\n",
    "t9 = torch.randn(10, 3, 4)\n",
    "t10 = torch.randn(4, 5)\n",
    "print(torch.matmul(t9, t10).size())  # 결과 크기: torch.Size([10, 3, 5])\n",
    "\n",
    "# 기술적 사항\n",
    "# 1. torch.matmul()은 다양한 형태의 텐서 간의 곱셈을 지원하며, 브로드캐스팅 규칙을 따라 연산을 수행한다.\n",
    "# 2. 벡터, 행렬, 그리고 배치 행렬 간의 연산을 통해 다양한 형태의 데이터 처리 가능.\n",
    "\n",
    "# 고찰\n",
    "# 1. 내적 및 행렬 곱셈은 데이터 과학 및 딥러닝에서 기본적인 연산으로, 이들을 이해하는 것이 중요하다.\n",
    "# 2. PyTorch의 연산은 효율적인 메모리 사용과 성능 최적화를 통해 대규모 데이터 처리를 가능하게 한다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da5562a-e8cc-421b-aaff-9dc29a6d9d1b",
   "metadata": {},
   "source": [
    "## i_tensor_boradcasting.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6ee96f38-2562-41e0-a5bb-9a923b160bb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2., 4., 6.])\n",
      "################################################## 1\n",
      "tensor([[-4, -4],\n",
      "        [-2, -1],\n",
      "        [ 6,  5]])\n",
      "################################################## 2\n",
      "tensor([[3., 4.],\n",
      "        [5., 6.]])\n",
      "tensor([[-1.,  0.],\n",
      "        [ 1.,  2.]])\n",
      "tensor([[2., 4.],\n",
      "        [6., 8.]])\n",
      "tensor([[0.5000, 1.0000],\n",
      "        [1.5000, 2.0000]])\n",
      "################################################## 3\n",
      "torch.Size([3, 28, 28])\n",
      "################################################## 4\n",
      "tensor([[4, 3],\n",
      "        [3, 4]])\n",
      "tensor([[6, 7],\n",
      "        [2, 5]])\n",
      "tensor([[8, 6],\n",
      "        [5, 3]])\n",
      "tensor([[ 8,  9],\n",
      "        [ 7, 10]])\n",
      "################################################## 5\n",
      "torch.Size([4, 3, 2])\n",
      "torch.Size([4, 3, 2])\n",
      "torch.Size([4, 3, 2])\n",
      "torch.Size([5, 3, 4, 1])\n",
      "################################################## 6\n",
      "torch.Size([5, 3, 4, 1])\n",
      "torch.Size([3, 1, 7])\n",
      "torch.Size([3, 3, 3])\n",
      "################################################## 7\n",
      "tensor([5., 5., 5., 5.])\n",
      "tensor([25., 25., 25., 25.])\n",
      "tensor([  1.,   4.,  27., 256.])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# [핵심코드] 스칼라와 벡터의 곱\n",
    "t1 = torch.tensor([1.0, 2.0, 3.0])\n",
    "t2 = 2.0\n",
    "print(t1 * t2)\n",
    "\n",
    "print(\"#\" * 50, 1)\n",
    "\n",
    "# [핵심코드] 행렬과 벡터의 뺄셈\n",
    "t3 = torch.tensor([[0, 1], [2, 4], [10, 10]])\n",
    "t4 = torch.tensor([4, 5])\n",
    "print(t3 - t4)\n",
    "\n",
    "print(\"#\" * 50, 2)\n",
    "\n",
    "# [핵심코드] 행렬에 스칼라를 더하고 빼고 곱하고 나누기\n",
    "t5 = torch.tensor([[1., 2.], [3., 4.]])\n",
    "print(t5 + 2.0)  # t5.add(2.0)\n",
    "print(t5 - 2.0)  # t5.sub(2.0)\n",
    "print(t5 * 2.0)  # t5.mul(2.0)\n",
    "print(t5 / 2.0)  # t5.div(2.0)\n",
    "\n",
    "print(\"#\" * 50, 3)\n",
    "\n",
    "# [핵심코드] 정규화 함수\n",
    "def normalize(x):\n",
    "  return x / 255\n",
    "\n",
    "t6 = torch.randn(3, 28, 28)\n",
    "print(normalize(t6).size())\n",
    "\n",
    "print(\"#\" * 50, 4)\n",
    "\n",
    "# [핵심코드] 다양한 텐서 간의 브로드캐스팅 연산\n",
    "t7 = torch.tensor([[1, 2], [0, 3]])  # torch.Size([2, 2])\n",
    "t8 = torch.tensor([[3, 1]])  # torch.Size([1, 2])\n",
    "t9 = torch.tensor([[5], [2]])  # torch.Size([2, 1])\n",
    "t10 = torch.tensor([7])  # torch.Size([1])\n",
    "print(t7 + t8)   # >>> tensor([[4, 3], [3, 4]])\n",
    "print(t7 + t9)   # >>> tensor([[6, 7], [2, 5]])\n",
    "print(t8 + t9)   # >>> tensor([[8, 6], [5, 3]])\n",
    "print(t7 + t10)  # >>> tensor([[ 8, 9], [ 7, 10]])\n",
    "\n",
    "print(\"#\" * 50, 5)\n",
    "\n",
    "# [핵심코드] 브로드캐스팅을 이용한 텐서 연산\n",
    "t11 = torch.ones(4, 3, 2)\n",
    "t12 = t11 * torch.rand(3, 2)  # 3rd & 2nd dims identical to t11, dim 0 absent\n",
    "print(t12.shape)\n",
    "\n",
    "t13 = torch.ones(4, 3, 2)\n",
    "t14 = t13 * torch.rand(3, 1)  # 3rd dim = 1, 2nd dim is identical to t13\n",
    "print(t14.shape)\n",
    "\n",
    "t15 = torch.ones(4, 3, 2)\n",
    "t16 = t15 * torch.rand(1, 2)  # 3rd dim is identical to t15, 2nd dim is 1\n",
    "print(t16.shape)\n",
    "\n",
    "t17 = torch.ones(5, 3, 4, 1)\n",
    "t18 = torch.rand(3, 1, 1)  # 2nd dim is identical to t17, 3rd and 4th dims are 1\n",
    "print((t17 + t18).size())\n",
    "\n",
    "print(\"#\" * 50, 6)\n",
    "\n",
    "# [핵심코드] 브로드캐스팅을 이용한 텐서 연산\n",
    "t19 = torch.empty(5, 1, 4, 1)\n",
    "t20 = torch.empty(3, 1, 1)\n",
    "print((t19 + t20).size())  # torch.Size([5, 3, 4, 1])\n",
    "\n",
    "t21 = torch.empty(1)\n",
    "t22 = torch.empty(3, 1, 7)\n",
    "print((t21 + t22).size())  # torch.Size([3, 1, 7])\n",
    "\n",
    "t23 = torch.ones(3, 3, 3)\n",
    "t24 = torch.ones(3, 1, 3)\n",
    "print((t23 + t24).size())  # torch.Size([3, 3, 3])\n",
    "\n",
    "# t25 = torch.empty(5, 2, 4, 1)\n",
    "# t26 = torch.empty(3, 1, 1)\n",
    "# print((t25 + t26).size())\n",
    "# RuntimeError: The size of tensor a (2) must match\n",
    "# the size of tensor b (3) at non-singleton dimension 1\n",
    "\n",
    "print(\"#\" * 50, 7)\n",
    "\n",
    "# [핵심코드] 텐서의 원소 제곱\n",
    "t27 = torch.ones(4) * 5\n",
    "print(t27)  # >>> tensor([ 5, 5, 5, 5])\n",
    "\n",
    "t28 = torch.pow(t27, 2)\n",
    "print(t28)  # >>> tensor([ 25, 25, 25, 25])\n",
    "\n",
    "# [핵심코드] 텐서의 원소 제곱\n",
    "exp = torch.arange(1., 5.)  # tensor([ 1.,  2.,  3.,  4.])\n",
    "a = torch.arange(1., 5.)  # tensor([ 1.,  2.,  3.,  4.])\n",
    "t29 = torch.pow(a, exp)\n",
    "print(t29)  # >>> tensor([   1.,    4.,   27.,  256.])\n",
    "# 기술적 사항\n",
    "# 1. PyTorch는 다양한 텐서 연산을 지원하며, 스칼라, 벡터, 행렬 간의 연산을 쉽게 수행할 수 있다.\n",
    "# 2. 브로드캐스팅 기능을 통해 서로 다른 크기의 텐서 간의 연산을 가능하게 한다.\n",
    "# 3. 다양한 텐서 조작 기능(예: add, sub, mul, div 등)을 제공하여 수학적 연산을 간편하게 처리할 수 있다.\n",
    "\n",
    "# 고찰\n",
    "# 1. 텐서 연산은 딥러닝 모델의 핵심 구성 요소로, 이러한 연산의 이해는 모델 개발 및 최적화에 매우 중요하다.\n",
    "# 2. 브로드캐스팅은 메모리 효율성을 높이고 코드의 가독성을 향상시키는 중요한 기능이다.\n",
    "# 3. PyTorch의 유연한 텐서 조작 기능을 활용하여 다양한 데이터 구조를 효과적으로 다룰 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c442818-3939-4007-adef-d67d2ff34410",
   "metadata": {},
   "source": [
    "## j_tensor_indexing_slicing.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3cf39447-9852-41b0-9dab-3c74bf3a718f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5, 6, 7, 8, 9])\n",
      "tensor([ 1,  6, 11])\n",
      "tensor(7)\n",
      "tensor([ 4,  9, 14])\n",
      "################################################## 1\n",
      "tensor([[ 5,  6,  7,  8,  9],\n",
      "        [10, 11, 12, 13, 14]])\n",
      "tensor([[ 8,  9],\n",
      "        [13, 14]])\n",
      "################################################## 2\n",
      "tensor([[0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.]])\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]])\n",
      "################################################## 3\n",
      "tensor([[1, 2, 3, 4],\n",
      "        [2, 3, 4, 5]])\n",
      "tensor([[3, 4],\n",
      "        [6, 7]])\n",
      "tensor([[2, 3, 4],\n",
      "        [3, 4, 5],\n",
      "        [6, 7, 8]])\n",
      "tensor([[1, 2, 3, 4],\n",
      "        [2, 0, 0, 5],\n",
      "        [5, 0, 0, 8]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# [핵심코드] 2D 텐서 생성\n",
    "x = torch.tensor(\n",
    "  [[0, 1, 2, 3, 4],\n",
    "   [5, 6, 7, 8, 9],\n",
    "   [10, 11, 12, 13, 14]]\n",
    ")\n",
    "\n",
    "# [핵심코드] 텐서 인덱싱\n",
    "print(x[1])  # >>> tensor([5, 6, 7, 8, 9])  # 1번째 행\n",
    "print(x[:, 1])  # >>> tensor([1, 6, 11])  # 1번째 열\n",
    "print(x[1, 2])  # >>> tensor(7)  # 1행 2열의 원소\n",
    "print(x[:, -1])  # >>> tensor([4, 9, 14])  # 마지막 열\n",
    "\n",
    "print(\"#\" * 50, 1)\n",
    "\n",
    "# [핵심코드] 슬라이싱\n",
    "print(x[1:])  # >>> tensor([[ 5,  6,  7,  8,  9], [10, 11, 12, 13, 14]])  # 1행부터 끝까지\n",
    "print(x[1:, 3:])  # >>> tensor([[ 8,  9], [13, 14]])  # 1행부터 끝까지, 3열부터 끝까지\n",
    "\n",
    "print(\"#\" * 50, 2)\n",
    "\n",
    "# [핵심코드] 부분 텐서 생성 및 값 할당\n",
    "y = torch.zeros((6, 6))  # 6x6 크기의 제로 텐서 생성\n",
    "y[1:4, 2] = 1  # 특정 부분에 값 할당\n",
    "print(y)\n",
    "\n",
    "print(y[1:4, 1:4])  # >>> tensor([[0., 1., 1.], [0., 1., 1.], [0., 1., 1.]])\n",
    "\n",
    "print(\"#\" * 50, 3)\n",
    "\n",
    "# [핵심코드] 2D 텐서 생성 및 인덱싱\n",
    "z = torch.tensor(\n",
    "  [[1, 2, 3, 4],\n",
    "   [2, 3, 4, 5],\n",
    "   [5, 6, 7, 8]]\n",
    ")\n",
    "print(z[:2])  # >>> tensor([[1, 2, 3, 4], [2, 3, 4, 5]])  # 처음 두 행\n",
    "print(z[1:, 1:3])  # >>> tensor([[3, 4], [6, 7]])  # 1행부터 끝까지, 1열부터 3열까지\n",
    "print(z[:, 1:])  # >>> tensor([[2, 3, 4], [3, 4, 5], [6, 7, 8]])  # 모든 행, 1열부터 끝까지\n",
    "\n",
    "# [핵심코드] 부분 텐서에 값 할당\n",
    "z[1:, 1:3] = 0  # 특정 부분에 값 할당\n",
    "print(z)  # >>> tensor([[1, 2, 3, 4], [2, 0, 0, 5], [5, 0, 0, 8]])\n",
    "\n",
    "# 기술적 사항\n",
    "# 1. PyTorch에서는 텐서의 인덱싱, 슬라이싱, 및 특정 부분에 값 할당을 통해 데이터를 쉽게 조작할 수 있다.\n",
    "# 2. 텐서의 행과 열을 선택하거나 특정 부분을 변경하는 기능은 데이터 처리 및 전처리에서 매우 유용하다.\n",
    "\n",
    "# 고찰\n",
    "# 1. 텐서 인덱싱과 슬라이싱은 데이터 분석 및 딥러닝 모델 개발 시 핵심적인 기술이다.\n",
    "# 2. 효율적인 데이터 조작을 통해 모델의 입력 데이터를 효과적으로 준비할 수 있다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd6579f-f367-4698-a74b-9346a6a69000",
   "metadata": {},
   "source": [
    "## k_tensor_reshaping.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "de832b6c-465d-4351-9044-c1460a284ae6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2],\n",
      "        [3, 4],\n",
      "        [5, 6]])\n",
      "tensor([[1, 2, 3, 4, 5, 6]])\n",
      "tensor([[0, 1, 2, 3],\n",
      "        [4, 5, 6, 7]])\n",
      "tensor([[0, 1, 2],\n",
      "        [3, 4, 5]])\n",
      "################################################## 1\n",
      "tensor([1, 2, 3])\n",
      "tensor([[1],\n",
      "        [2],\n",
      "        [3]])\n",
      "################################################## 2\n",
      "tensor([[1],\n",
      "        [2],\n",
      "        [3]])\n",
      "tensor([[[1, 2, 3]],\n",
      "\n",
      "        [[4, 5, 6]]]) torch.Size([2, 1, 3])\n",
      "################################################## 3\n",
      "tensor([1, 2, 3, 4, 5, 6])\n",
      "tensor([1, 2, 3, 4, 5, 6, 7, 8])\n",
      "tensor([[1, 2, 3, 4],\n",
      "        [5, 6, 7, 8]])\n",
      "################################################## 4\n",
      "torch.Size([2, 3, 5])\n",
      "torch.Size([5, 2, 3])\n",
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n",
      "tensor([[1, 4],\n",
      "        [2, 5],\n",
      "        [3, 6]])\n",
      "tensor([[1, 4],\n",
      "        [2, 5],\n",
      "        [3, 6]])\n",
      "tensor([[1, 4],\n",
      "        [2, 5],\n",
      "        [3, 6]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# [핵심코드] 텐서 생성 및 모양 변경\n",
    "t1 = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "t2 = t1.view(3, 2)  # Shape becomes (3, 2)\n",
    "t3 = t1.reshape(1, 6)  # Shape becomes (1, 6)\n",
    "print(t2)\n",
    "print(t3)\n",
    "\n",
    "# [핵심코드] 텐서 생성\n",
    "t4 = torch.arange(8).view(2, 4)  # Shape becomes (2, 4)\n",
    "t5 = torch.arange(6).view(2, 3)  # Shape becomes (2, 3)\n",
    "print(t4)\n",
    "print(t5)\n",
    "\n",
    "print(\"#\" * 50, 1)\n",
    "\n",
    "# [핵심코드] 차원 축소\n",
    "t6 = torch.tensor([[[1], [2], [3]]])  # Original tensor with shape (1, 3, 1)\n",
    "\n",
    "t7 = t6.squeeze()  # Shape becomes (3,)  # 모든 차원에서 크기가 1인 차원 제거\n",
    "t8 = t6.squeeze(0)  # Shape becomes (3, 1)  # 0번째 위치의 차원 제거\n",
    "print(t7)\n",
    "print(t8)\n",
    "\n",
    "print(\"#\" * 50, 2)\n",
    "\n",
    "# [핵심코드] 차원 추가\n",
    "t9 = torch.tensor([1, 2, 3])  # Original tensor with shape (3,)\n",
    "\n",
    "t10 = t9.unsqueeze(1)  # Shape becomes (3, 1)  # 1번째 위치에 새로운 차원 추가\n",
    "print(t10)\n",
    "\n",
    "t11 = torch.tensor(\n",
    "  [[1, 2, 3],\n",
    "   [4, 5, 6]]\n",
    ")\n",
    "t12 = t11.unsqueeze(1)  # Shape becomes (2, 1, 3)  # 1번째 위치에 새로운 차원 추가\n",
    "print(t12, t12.shape)\n",
    "\n",
    "print(\"#\" * 50, 3)\n",
    "\n",
    "# [핵심코드] 텐서 평탄화\n",
    "t13 = torch.tensor([[1, 2, 3], [4, 5, 6]])  # Original tensor with shape (2, 3)\n",
    "\n",
    "t14 = t13.flatten()  # Shape becomes (6,)  # 텐서 평탄화\n",
    "print(t14)\n",
    "\n",
    "# Original tensor with shape (2, 2, 2)\n",
    "t15 = torch.tensor([[[1, 2],\n",
    "                     [3, 4]],\n",
    "                    [[5, 6],\n",
    "                     [7, 8]]])\n",
    "t16 = torch.flatten(t15)  # 평탄화\n",
    "t17 = torch.flatten(t15, start_dim=1)  # 첫 번째 차원부터 평탄화\n",
    "\n",
    "print(t16)  # Shape becomes (8,)\n",
    "print(t17)  # Shape becomes (2, 4)\n",
    "\n",
    "print(\"#\" * 50, 4)\n",
    "\n",
    "# [핵심코드] 차원 순서 변경\n",
    "t18 = torch.randn(2, 3, 5)\n",
    "print(t18.shape)  # >>> torch.Size([2, 3, 5])\n",
    "print(torch.permute(t18, (2, 0, 1)).size())  # >>> torch.Size([5, 2, 3])\n",
    "\n",
    "# Original tensor with shape (2, 3)\n",
    "t19 = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "\n",
    "# [핵심코드] 차원 순서 변경\n",
    "t20 = torch.permute(t19, dims=(0, 1))  # Shape becomes (2, 3) still\n",
    "t21 = torch.permute(t19, dims=(1, 0))  # Shape becomes (3, 2)\n",
    "print(t20)\n",
    "print(t21)\n",
    "\n",
    "# [핵심코드] 텐서 전치\n",
    "t22 = torch.transpose(t19, 0, 1)  # Shape becomes (3, 2)\n",
    "print(t22)\n",
    "\n",
    "t23 = torch.t(t19)  # Shape becomes (3, 2)\n",
    "print(t23)\n",
    "\n",
    "# 기술적 사항\n",
    "# 1. PyTorch의 view()와 reshape() 함수는 텐서의 형태를 변경하는 데 사용된다.\n",
    "# 2. squeeze()와 unsqueeze() 함수는 차원을 제거하거나 추가하는 데 사용된다.\n",
    "# 3. flatten() 함수는 텐서를 평탄화하여 1차원으로 변환한다.\n",
    "# 4. permute()와 transpose() 함수는 텐서의 차원 순서를 변경한다.\n",
    "\n",
    "# 고찰\n",
    "# 1. 텐서의 차원 조작은 딥러닝 모델에서 데이터 전처리 및 배치 처리에 매우 중요하다.\n",
    "# 2. PyTorch의 다양한 텐서 조작 기능을 활용하여 복잡한 구조의 데이터를 효율적으로 다룰 수 있다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a6726eb-e074-4c9b-8a9a-33892c45157b",
   "metadata": {},
   "source": [
    "## l_tensor_concat.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f177465e-6349-4280-be91-af651bc08f8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 6, 3])\n",
      "################################################## 1\n",
      "torch.Size([8])\n",
      "tensor([0, 1, 2, 3, 4, 5, 6, 7])\n",
      "################################################## 2\n",
      "torch.Size([4, 3])\n",
      "tensor([[ 0,  1,  2],\n",
      "        [ 3,  4,  5],\n",
      "        [ 6,  7,  8],\n",
      "        [ 9, 10, 11]])\n",
      "torch.Size([2, 6])\n",
      "tensor([[ 0,  1,  2,  6,  7,  8],\n",
      "        [ 3,  4,  5,  9, 10, 11]])\n",
      "################################################## 3\n",
      "torch.Size([6, 3])\n",
      "tensor([[ 0,  1,  2],\n",
      "        [ 3,  4,  5],\n",
      "        [ 6,  7,  8],\n",
      "        [ 9, 10, 11],\n",
      "        [12, 13, 14],\n",
      "        [15, 16, 17]])\n",
      "torch.Size([2, 9])\n",
      "tensor([[ 0,  1,  2,  6,  7,  8, 12, 13, 14],\n",
      "        [ 3,  4,  5,  9, 10, 11, 15, 16, 17]])\n",
      "################################################## 4\n",
      "torch.Size([2, 2, 3])\n",
      "tensor([[[ 0,  1,  2],\n",
      "         [ 3,  4,  5]],\n",
      "\n",
      "        [[ 6,  7,  8],\n",
      "         [ 9, 10, 11]]])\n",
      "torch.Size([1, 4, 3])\n",
      "tensor([[[ 0,  1,  2],\n",
      "         [ 3,  4,  5],\n",
      "         [ 6,  7,  8],\n",
      "         [ 9, 10, 11]]])\n",
      "torch.Size([1, 2, 6])\n",
      "tensor([[[ 0,  1,  2,  6,  7,  8],\n",
      "         [ 3,  4,  5,  9, 10, 11]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# [핵심코드] 다양한 크기의 텐서 생성\n",
    "t1 = torch.zeros([2, 1, 3])\n",
    "t2 = torch.zeros([2, 3, 3])\n",
    "t3 = torch.zeros([2, 2, 3])\n",
    "\n",
    "# [핵심코드] 텐서 병합\n",
    "t4 = torch.cat([t1, t2, t3], dim=1)\n",
    "print(t4.shape)  # Shape 확인\n",
    "\n",
    "print(\"#\" * 50, 1)\n",
    "\n",
    "# [핵심코드] 1D 텐서 생성 및 병합\n",
    "t5 = torch.arange(0, 3)  # tensor([0, 1, 2])\n",
    "t6 = torch.arange(3, 8)  # tensor([3, 4, 5, 6, 7])\n",
    "\n",
    "t7 = torch.cat((t5, t6), dim=0)  # 병합\n",
    "print(t7.shape)  # >>> torch.Size([8])\n",
    "print(t7)  # >>> tensor([0, 1, 2, 3, 4, 5, 6, 7])\n",
    "\n",
    "print(\"#\" * 50, 2)\n",
    "\n",
    "# [핵심코드] 2D 텐서 생성 및 병합\n",
    "t8 = torch.arange(0, 6).reshape(2, 3)  # torch.Size([2, 3])\n",
    "t9 = torch.arange(6, 12).reshape(2, 3)  # torch.Size([2, 3])\n",
    "\n",
    "# 2차원 텐서간 병합\n",
    "t10 = torch.cat((t8, t9), dim=0)  # 행 방향으로 병합\n",
    "print(t10.size())  # >>> torch.Size([4, 3])\n",
    "print(t10)\n",
    "# >>> tensor([[ 0,  1,  2],\n",
    "#             [ 3,  4,  5],\n",
    "#             [ 6,  7,  8],\n",
    "#             [ 9, 10, 11]])\n",
    "\n",
    "t11 = torch.cat((t8, t9), dim=1)  # 열 방향으로 병합\n",
    "print(t11.size())  # >>> torch.Size([2, 6])\n",
    "print(t11)\n",
    "# >>> tensor([[ 0,  1,  2,  6,  7,  8],\n",
    "#             [ 3,  4,  5,  9, 10, 11]])\n",
    "\n",
    "print(\"#\" * 50, 3)\n",
    "\n",
    "# [핵심코드] 여러 2D 텐서 병합\n",
    "t12 = torch.arange(0, 6).reshape(2, 3)  # torch.Size([2, 3])\n",
    "t13 = torch.arange(6, 12).reshape(2, 3)  # torch.Size([2, 3])\n",
    "t14 = torch.arange(12, 18).reshape(2, 3)  # torch.Size([2, 3])\n",
    "\n",
    "t15 = torch.cat((t12, t13, t14), dim=0)  # 행 방향으로 병합\n",
    "print(t15.size())  # >>> torch.Size([6, 3])\n",
    "print(t15)\n",
    "# >>> tensor([[ 0,  1,  2],\n",
    "#             [ 3,  4,  5],\n",
    "#             [ 6,  7,  8],\n",
    "#             [ 9, 10, 11],\n",
    "#             [12, 13, 14],\n",
    "#             [15, 16, 17]])\n",
    "\n",
    "t16 = torch.cat((t12, t13, t14), dim=1)  # 열 방향으로 병합\n",
    "print(t16.size())  # >>> torch.Size([2, 9])\n",
    "print(t16)\n",
    "# >>> tensor([[ 0,  1,  2,  6,  7,  8, 12, 13, 14],\n",
    "#             [ 3,  4,  5,  9, 10, 11, 15, 16, 17]])\n",
    "\n",
    "print(\"#\" * 50, 4)\n",
    "\n",
    "# [핵심코드] 3D 텐서 생성 및 병합\n",
    "t17 = torch.arange(0, 6).reshape(1, 2, 3)  # torch.Size([1, 2, 3])\n",
    "t18 = torch.arange(6, 12).reshape(1, 2, 3)  # torch.Size([1, 2, 3])\n",
    "\n",
    "t19 = torch.cat((t17, t18), dim=0)  # 첫 번째 차원에서 병합\n",
    "print(t19.size())  # >>> torch.Size([2, 2, 3])\n",
    "print(t19)\n",
    "# >>> tensor([[[ 0,  1,  2],\n",
    "#              [ 3,  4,  5]],\n",
    "#             [[ 6,  7,  8],\n",
    "#              [ 9, 10, 11]]])\n",
    "\n",
    "t20 = torch.cat((t17, t18), dim=1)  # 두 번째 차원에서 병합\n",
    "print(t20.size())  # >>> torch.Size([1, 4, 3])\n",
    "print(t20)\n",
    "# >>> tensor([[[ 0,  1,  2],\n",
    "#              [ 3,  4,  5],\n",
    "#              [ 6,  7,  8],\n",
    "#              [ 9, 10, 11]]])\n",
    "\n",
    "t21 = torch.cat((t17, t18), dim=2)  # 세 번째 차원에서 병합\n",
    "print(t21.size())  # >>> torch.Size([1, 2, 6])\n",
    "print(t21)\n",
    "# >>> tensor([[[ 0,  1,  2,  6,  7,  8],\n",
    "#              [ 3,  4,  5,  9, 10, 11]]])\n",
    "\n",
    "# 기술적 사항\n",
    "# 1. torch.cat() 함수는 여러 텐서를 특정 차원에서 연결하는 데 사용된다.\n",
    "# 2. 텐서를 병합할 때는 연결할 차원의 크기가 동일해야 한다.\n",
    "# 3. 다양한 차원에서 텐서를 병합할 수 있어 유연한 데이터 구조를 생성할 수 있다.\n",
    "\n",
    "# 고찰\n",
    "# 1. 텐서 병합은 딥러닝 모델의 입력 데이터를 준비하는 데 필수적인 과정이다.\n",
    "# 2. PyTorch의 텐서 조작 기능을 활용하여 복잡한 데이터 구조를 효과적으로 처리할 수 있다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "878215b6-cd2a-4fb4-8052-2f64e169646e",
   "metadata": {},
   "source": [
    "## m_tensor_stacking.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5699e97d-d40e-4aa3-9639-756e22161353",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2, 3]) True\n",
      "torch.Size([2, 2, 3]) True\n",
      "torch.Size([2, 3, 2]) True\n",
      "################################################## 1\n",
      "torch.Size([3]) torch.Size([3])\n",
      "torch.Size([2, 3])\n",
      "tensor([[0, 1, 2],\n",
      "        [3, 4, 5]])\n",
      "True\n",
      "torch.Size([3, 2])\n",
      "tensor([[0, 3],\n",
      "        [1, 4],\n",
      "        [2, 5]])\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# [핵심코드] 텐서 생성\n",
    "t1 = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "t2 = torch.tensor([[7, 8, 9], [10, 11, 12]])\n",
    "\n",
    "# [핵심코드] 텐서 스택 및 연결\n",
    "t3 = torch.stack([t1, t2], dim=0)  # 차원 0에서 스택\n",
    "t4 = torch.cat([t1.unsqueeze(dim=0), t2.unsqueeze(dim=0)], dim=0)  # 차원 0에서 연결\n",
    "print(t3.shape, t3.equal(t4))  # 모양 및 값 비교\n",
    "\n",
    "# [핵심코드] 차원 1에서 스택 및 연결\n",
    "t5 = torch.stack([t1, t2], dim=1)  # 차원 1에서 스택\n",
    "t6 = torch.cat([t1.unsqueeze(dim=1), t2.unsqueeze(dim=1)], dim=1)  # 차원 1에서 연결\n",
    "print(t5.shape, t5.equal(t6))  # 모양 및 값 비교\n",
    "\n",
    "# [핵심코드] 차원 2에서 스택 및 연결\n",
    "t7 = torch.stack([t1, t2], dim=2)  # 차원 2에서 스택\n",
    "t8 = torch.cat([t1.unsqueeze(dim=2), t2.unsqueeze(dim=2)], dim=2)  # 차원 2에서 연결\n",
    "print(t7.shape, t7.equal(t8))  # 모양 및 값 비교\n",
    "\n",
    "print(\"#\" * 50, 1)\n",
    "\n",
    "# [핵심코드] 1D 텐서 생성\n",
    "t9 = torch.arange(0, 3)  # tensor([0, 1, 2])\n",
    "t10 = torch.arange(3, 6)  # tensor([3, 4, 5])\n",
    "\n",
    "print(t9.size(), t10.size())  # 크기 출력\n",
    "# >>> torch.Size([3]) torch.Size([3])\n",
    "\n",
    "# [핵심코드] 1D 텐서 스택\n",
    "t11 = torch.stack((t9, t10), dim=0)  # 차원 0에서 스택\n",
    "print(t11.size())  # >>> torch.Size([2,3])\n",
    "print(t11)\n",
    "# >>> tensor([[0, 1, 2],\n",
    "#             [3, 4, 5]])\n",
    "\n",
    "# [핵심코드] 1D 텐서 연결\n",
    "t12 = torch.cat((t9.unsqueeze(0), t10.unsqueeze(0)), dim=0)  # 차원 0에서 연결\n",
    "print(t11.equal(t12))  # 값 비교\n",
    "# >>> True\n",
    "\n",
    "# [핵심코드] 차원 1에서 스택\n",
    "t13 = torch.stack((t9, t10), dim=1)  # 차원 1에서 스택\n",
    "print(t13.size())  # >>> torch.Size([3,2])\n",
    "print(t13)\n",
    "# >>> tensor([[0, 3],\n",
    "#             [1, 4],\n",
    "#             [2, 5]])\n",
    "\n",
    "# [핵심코드] 차원 1에서 연결\n",
    "t14 = torch.cat((t9.unsqueeze(1), t10.unsqueeze(1)), dim=1)  # 차원 1에서 연결\n",
    "print(t13.equal(t14))  # 값 비교\n",
    "# >>> True\n",
    "\n",
    "# 기술적 사항\n",
    "# 1. torch.stack() 함수는 여러 텐서를 새로운 차원에서 스택하여 하나의 텐서로 만든다.\n",
    "# 2. torch.cat() 함수는 여러 텐서를 주어진 차원에서 연결한다.\n",
    "# 3. 두 함수 모두 텐서를 다루는 데 유용하며, 데이터의 형태를 조정할 수 있다.\n",
    "\n",
    "# 고찰\n",
    "# 1. 텐서의 차원 조작은 딥러닝 모델에서 데이터 입력 형식을 설정하는 데 매우 중요하다.\n",
    "# 2. PyTorch의 이러한 기능을 활용하여 다양한 형태의 데이터를 효과적으로 처리할 수 있다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1c5dc4-3a41-476b-b091-4693210aca35",
   "metadata": {},
   "source": [
    "## n_tensor_vstack_hstack.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4543b6cf-2147-4aa8-a1b4-f43db95f801f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n",
      "torch.Size([2, 2, 3])\n",
      "torch.Size([2, 2, 3])\n",
      "torch.Size([4, 2, 3])\n",
      "tensor([[[ 1,  2,  3],\n",
      "         [ 4,  5,  6]],\n",
      "\n",
      "        [[ 7,  8,  9],\n",
      "         [10, 11, 12]],\n",
      "\n",
      "        [[13, 14, 15],\n",
      "         [16, 17, 18]],\n",
      "\n",
      "        [[19, 20, 21],\n",
      "         [22, 23, 24]]])\n",
      "################################################## 1\n",
      "tensor([1, 2, 3, 4, 5, 6])\n",
      "tensor([[1, 4],\n",
      "        [2, 5],\n",
      "        [3, 6]])\n",
      "torch.Size([2, 2, 3])\n",
      "torch.Size([2, 2, 3])\n",
      "torch.Size([2, 4, 3])\n",
      "tensor([[[ 1,  2,  3],\n",
      "         [ 4,  5,  6],\n",
      "         [13, 14, 15],\n",
      "         [16, 17, 18]],\n",
      "\n",
      "        [[ 7,  8,  9],\n",
      "         [10, 11, 12],\n",
      "         [19, 20, 21],\n",
      "         [22, 23, 24]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# [핵심코드] 1D 텐서 생성 및 수직 스택\n",
    "t1 = torch.tensor([1, 2, 3])\n",
    "t2 = torch.tensor([4, 5, 6])\n",
    "t3 = torch.vstack((t1, t2))  # 수직으로 스택\n",
    "print(t3)\n",
    "# >>> tensor([[1, 2, 3],\n",
    "#             [4, 5, 6]])\n",
    "\n",
    "# [핵심코드] 2D 텐서 생성 및 수직 스택\n",
    "t4 = torch.tensor([[1], [2], [3]])\n",
    "t5 = torch.tensor([[4], [5], [6]])\n",
    "t6 = torch.vstack((t4, t5))  # 수직으로 스택\n",
    "# >>> tensor([[1],\n",
    "#             [2],\n",
    "#             [3],\n",
    "#             [4],\n",
    "#             [5],\n",
    "#             [6]])\n",
    "\n",
    "# [핵심코드] 3D 텐서 생성\n",
    "t7 = torch.tensor([\n",
    "  [[1, 2, 3], [4, 5, 6]],\n",
    "  [[7, 8, 9], [10, 11, 12]]\n",
    "])\n",
    "print(t7.shape)  # >>> (2, 2, 3)\n",
    "\n",
    "t8 = torch.tensor([\n",
    "  [[13, 14, 15], [16, 17, 18]],\n",
    "  [[19, 20, 21], [22, 23, 24]]\n",
    "])\n",
    "print(t8.shape)  # >>> (2, 2, 3)\n",
    "\n",
    "# [핵심코드] 3D 텐서 수직 스택\n",
    "t9 = torch.vstack([t7, t8])  # 수직으로 스택\n",
    "print(t9.shape)  # >>> (4, 2, 3)\n",
    "\n",
    "print(t9)\n",
    "# >>> tensor([[[ 1,  2,  3],\n",
    "#              [ 4,  5,  6]],\n",
    "#             [[ 7,  8,  9],\n",
    "#              [10, 11, 12]],\n",
    "#             [[13, 14, 15],\n",
    "#              [16, 17, 18]],\n",
    "#             [[19, 20, 21],\n",
    "#              [22, 23, 24]]])\n",
    "\n",
    "print(\"#\" * 50, 1)\n",
    "\n",
    "# [핵심코드] 1D 텐서 수평 스택\n",
    "t10 = torch.tensor([1, 2, 3])\n",
    "t11 = torch.tensor([4, 5, 6])\n",
    "t12 = torch.hstack((t10, t11))  # 수평으로 스택\n",
    "print(t12)\n",
    "# >>> tensor([1, 2, 3, 4, 5, 6])\n",
    "\n",
    "# [핵심코드] 2D 텐서 수평 스택\n",
    "t13 = torch.tensor([[1], [2], [3]])\n",
    "t14 = torch.tensor([[4], [5], [6]])\n",
    "t15 = torch.hstack((t13, t14))  # 수평으로 스택\n",
    "print(t15)\n",
    "# >>> tensor([[1, 4],\n",
    "#             [2, 5],\n",
    "#             [3, 6]])\n",
    "\n",
    "# [핵심코드] 3D 텐서 생성\n",
    "t16 = torch.tensor([\n",
    "  [[1, 2, 3], [4, 5, 6]],\n",
    "  [[7, 8, 9], [10, 11, 12]]\n",
    "])\n",
    "print(t16.shape)  # >>> (2, 2, 3)\n",
    "\n",
    "t17 = torch.tensor([\n",
    "  [[13, 14, 15], [16, 17, 18]],\n",
    "  [[19, 20, 21], [22, 23, 24]]\n",
    "])\n",
    "print(t17.shape)  # >>> (2, 2, 3)\n",
    "\n",
    "# [핵심코드] 3D 텐서 수평 스택\n",
    "t18 = torch.hstack([t16, t17])  # 수평으로 스택\n",
    "print(t18.shape)  # >>> (2, 4, 3)\n",
    "\n",
    "print(t18)\n",
    "# >>> tensor([[[ 1,  2,  3],\n",
    "#              [ 4,  5,  6],\n",
    "#              [13, 14, 15],\n",
    "#              [16, 17, 18]],\n",
    "#             [[ 7,  8,  9],\n",
    "#              [10, 11, 12],\n",
    "#              [19, 20, 21],\n",
    "#              [22, 23, 24]]])\n",
    "\n",
    "# 기술적 사항\n",
    "# 1. torch.vstack() 함수는 텐서를 수직으로 스택하여 새로운 차원을 추가한다.\n",
    "# 2. torch.hstack() 함수는 텐서를 수평으로 스택하여 새로운 차원을 추가한다.\n",
    "# 3. 이 두 함수는 다차원 텐서를 다룰 때 유용하며, 데이터의 구조를 쉽게 변형할 수 있다.\n",
    "\n",
    "# 고찰\n",
    "# 1. 텐서의 스택 및 연결은 딥러닝 모델의 입력 데이터를 준비하는 데 중요한 과정이다.\n",
    "# 2. PyTorch의 다양한 텐서 조작 기능을 활용하여 복잡한 데이터 구조를 효과적으로 처리할 수 있다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb6f77e-d2ac-4355-90eb-582997295346",
   "metadata": {},
   "source": [
    "[숙제 후기]\n",
    "\n",
    "이번 학습을 통해 PyTorch의 텐서 조작에 대한 이해도를 높일 수 있었습니다. 특히, 텐서 생성, 변형, 병합 등 다양한 연산을 직접 실습하며 데이터 전처리에 필수적인 기술들을 익혔습니다.\n",
    "\n",
    "텐서 조작의 중요성을 다시금 느꼈습니다. 모델에 입력되는 데이터의 형태가 잘못되면 모델 성능에 큰 영향을 미치기 때문에, 텐서 조작은 딥러닝 모델 개발에서 필수적인 과정입니다.\n",
    "\n",
    "스택과 연결의 차이를 명확히 이해하게 되었습니다. torch.vstack()과 torch.hstack() 함수를 활용하여 텐서를 수직 또는 수평으로 병합하는 방법을 익히면서, 텐서의 차원을 자유롭게 조절할 수 있다는 점에 놀랐습니다.\n",
    "\n",
    "브로드캐스팅과 차원 조작 역시 중요한 개념이었습니다. unsqueeze()와 squeeze() 함수를 통해 텐서의 차원을 추가하거나 제거하는 방법을 배우면서, 데이터를 모델에 맞게 변형하는 기술을 익혔습니다.\n",
    "\n",
    "PyTorch의 유연성에 감탄했습니다. 다양한 텐서 연산을 지원하여 복잡한 데이터 구조도 쉽게 처리할 수 있다는 점이 PyTorch의 큰 장점이라고 생각합니다.\n",
    "\n",
    "실습의 중요성을 다시 한번 확인했습니다. 이론적인 지식도 중요하지만, 실제로 코드를 작성하며 다양한 상황에 적용해 보는 것이 더욱 효과적이라는 것을 느꼈습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a2a85f-8d88-411c-8a8a-5d554a285788",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "link_dl",
   "language": "python",
   "name": "link_dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
